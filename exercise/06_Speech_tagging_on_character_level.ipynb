{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "char2idx = {}\n",
    "tag2idx = {}\n",
    "\n",
    "def prepare_char_seq(word, char2idx):\n",
    "    idxs = []\n",
    "    for char in word:\n",
    "        idxs.append(char2idx[char])\n",
    "        \n",
    "    return idxs\n",
    "\n",
    "def prepare_seq(seq, word2idx, char2idx):\n",
    "    idxs = []\n",
    "    for word in seq:\n",
    "        idxs.append((word2idx[word], prepare_char_seq(word, char2idx)))\n",
    "    \n",
    "    return idxs\n",
    "\n",
    "def preprare_tag(tag, tag2idx):\n",
    "    idxs = []\n",
    "    for t in tag:\n",
    "        idxs.append(tag2idx[t])\n",
    "    \n",
    "    return torch.LongTensor(idxs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "{'T': 0, 'h': 1, 'e': 2, 'd': 3, 'o': 4, 'g': 5, 'a': 6, 't': 7, 'p': 8, 'l': 9, 'E': 10, 'v': 11, 'r': 12, 'y': 13, 'b': 14, 'k': 15}\n"
     ]
    }
   ],
   "source": [
    "for sent, tags in train_data:\n",
    "    for word in sent:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)\n",
    "        for char in word:\n",
    "            if char not in char2idx:\n",
    "                char2idx[char] = len(char2idx)\n",
    "                \n",
    "print(word2idx)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embedding_dim = 3\n",
    "char_hidden_dim = 3\n",
    "word_embedding_dim = 6\n",
    "hidden_dim = 6\n",
    "word_vocab_size = len(word2idx)\n",
    "char_vocab_size = len(char2idx)\n",
    "tagset_size = len(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, word_embedding_dim, char_embedding_dim, \n",
    "                 hidden_dim, char_hidden_dim,\n",
    "                word_vocab_size, char_vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        \n",
    "        self.word_embedding = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
    "        self.word_lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.hidden_to_tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        \n",
    "        self.hidden = self.init_hidden(hidden_dim)\n",
    "        self.char_hidden = self.init_hidden(char_hidden_dim)\n",
    "        \n",
    "    def init_hidden(self, dim):\n",
    "        return (Variable(torch.zeros(1, 1, dim), requires_grad=True), \n",
    "                Variable(torch.zeros(1, 1, dim), requires_grad=True))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "#         print(\"sentence:\", sentence)\n",
    "        \n",
    "        word_idxs = []\n",
    "        lstm_char_result = []\n",
    "        for word in sentence:\n",
    "#             print(\"word:\", word)\n",
    "#             print(\"word[0]:\", word[0]) # word idx\n",
    "#             print(\"word[1]:\", word[1]) # char idxs\n",
    "            \n",
    "            word_idxs.append(word[0])\n",
    "            \n",
    "            char_idx = torch.LongTensor(word[1]).to(device)\n",
    "            char_embed = self.char_embedding(char_idx)\n",
    "#             print(\"char_embed:\", char_embed.shape)\n",
    "            \n",
    "            char_embed = char_embed.view(len(word[1]), 1, char_embedding_dim)\n",
    "#             print(\"reshaped char_embed:\", char_embed.shape)\n",
    "            \n",
    "            lstm_char_out, self.char_hidden = self.char_lstm(char_embed, self.char_hidden)\n",
    "#             print(\"lstm_char_out:\", lstm_char_out.shape)\n",
    "            \n",
    "#             print(\"lstm_char_out[-1]:\", lstm_char_out[-1])\n",
    "            \n",
    "            lstm_char_result.append(lstm_char_out[-1])\n",
    "            \n",
    "        word_idxs = torch.LongTensor(word_idxs).to(device)\n",
    "#         print(\"word_idxs:\", word_idxs.shape)\n",
    "        \n",
    "        lstm_char_result = torch.stack(lstm_char_result)\n",
    "#         print(\"lstm_char_result:\", lstm_char_result.shape)\n",
    "        \n",
    "        \n",
    "        word_embed = self.word_embedding(word_idxs)\n",
    "#         print(\"word_embed:\", word_embed.shape)\n",
    "        \n",
    "        word_embed = word_embed.view(len(sentence), 1, self.word_embedding_dim)\n",
    "#         print(\"reshaped word_embed:\", word_embed.shape)\n",
    "        \n",
    "        lstm_in = torch.cat((word_embed, lstm_char_result), dim=2)\n",
    "#         print(\"lstm_in:\", lstm_in.shape)\n",
    "        \n",
    "        \n",
    "        lstm_out, self.hidden = self.word_lstm(lstm_in, self.hidden)\n",
    "        \n",
    "        tag = self.hidden_to_tag(lstm_out.view(len(sentence), -1))\n",
    "        out = F.log_softmax(tag)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMTagger(\n",
      "  (char_embedding): Embedding(16, 3)\n",
      "  (char_lstm): LSTM(3, 3)\n",
      "  (word_embedding): Embedding(9, 6)\n",
      "  (word_lstm): LSTM(9, 6)\n",
      "  (hidden_to_tag): Linear(in_features=6, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(word_embedding_dim, char_embedding_dim,\n",
    "                   hidden_dim, char_hidden_dim,\n",
    "                   word_vocab_size, char_vocab_size, tagset_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor(1.1117)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\young\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-59081c78342f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    for step, (sentence, tags) in enumerate(train_data):\n",
    "        \n",
    "        print(step)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        inputs = prepare_seq(sentence, word2idx, char2idx)\n",
    "        targets = preprare_tag(tags, tag2idx)\n",
    "\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, targets)\n",
    "        print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
