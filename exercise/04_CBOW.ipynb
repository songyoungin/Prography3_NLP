{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26eeb905a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    }
   ],
   "source": [
    "context_size = 2 # target 앞 뒤로 2개의 단어를 참조\n",
    "raw_sentence =  \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "print(len(raw_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(raw_sentence)\n",
    "word2idx = {w: idx for idx, w in enumerate(vocab)}\n",
    "idx2word = {idx: w for w, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10\n",
    "data = []\n",
    "for i in range(2, len(raw_sentence)-2):\n",
    "    context = [raw_sentence[i-2], raw_sentence[i-1],\n",
    "              raw_sentence[i+1], raw_sentence[i+2]]\n",
    "    target = [raw_sentence[i]]\n",
    "    data.append((context, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'to', 'study']\n",
      "['about']\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0]) # context\n",
    "print(data[0][1]) # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.linear1(self.embedding(x).view(1, -1)))\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW(\n",
      "  (embedding): Embedding(49, 10)\n",
      "  (linear1): Linear(in_features=40, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=49, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embedding_dim, context_size*2) # 앞 뒤로 2개씩 들어가서\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector to index vector\n",
    "def wordvec_to_idxvec(vec, word2idx):\n",
    "    idxs = [word2idx[w] for w in vec]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/10] loss:3.972\n",
      "[2/10] loss:2.813\n",
      "[3/10] loss:1.250\n",
      "[4/10] loss:0.122\n",
      "[5/10] loss:0.026\n",
      "[6/10] loss:0.001\n",
      "[7/10] loss:0.008\n",
      "[8/10] loss:0.000\n",
      "[9/10] loss:0.000\n",
      "[10/10] loss:0.000\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    for step, (context, target) in enumerate(data):\n",
    "        context = wordvec_to_idxvec(context, word2idx)\n",
    "        target = wordvec_to_idxvec(target, word2idx)\n",
    "        \n",
    "        context = torch.LongTensor(context)\n",
    "        target = torch.LongTensor(target)\n",
    "        \n",
    "        outputs = model(context)\n",
    "        loss = criterion(outputs, target)\n",
    "        \n",
    "#         print(outputs)\n",
    "#         print(target)\n",
    "        \n",
    "#         print(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    print(\"[%d/%d] loss:%.3f\" % (epoch+1, 10, np.mean(losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['processes', 'are', 'beings', 'that']\n",
      "['abstract']\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "test_sentence = \"processes are abstract beings that\"\n",
    "test_sentence = test_sentence.split()\n",
    "test_context = [test_sentence[0], test_sentence[1], test_sentence[3], test_sentence[4]]\n",
    "test_target = [test_sentence[2]]\n",
    "\n",
    "print(test_context)\n",
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  8,  25,   4,  19])\n",
      "tensor([ 27])\n"
     ]
    }
   ],
   "source": [
    "context_vec = wordvec_to_idxvec(test_context, word2idx)\n",
    "target_vec = wordvec_to_idxvec(test_target, word2idx)\n",
    "context_vec, target_vec = torch.LongTensor(context_vec), torch.LongTensor(target_vec)\n",
    "print(context_vec)\n",
    "print(target_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result: abstract\n",
      "Actual target: abstract\n"
     ]
    }
   ],
   "source": [
    "outputs = model(context_vec)\n",
    "pred, idx = torch.max(outputs, dim=1)\n",
    "\n",
    "result = idx2word[idx.item()]\n",
    "print(\"Predicted result:\", result)\n",
    "print(\"Actual target:\", test_target[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
