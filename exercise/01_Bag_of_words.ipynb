{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "kor_tagger = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'my', 'name', 'is', 'Youngin', '.', 'What', \"'s\", 'your', 'name', '?']\n"
     ]
    }
   ],
   "source": [
    "eng = nltk.word_tokenize(\"Hi, my name is Youngin. What's your name?\")\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '하', '세요', '!', '저', '는', '파이', '토치', '를', '공부', '하', '는', '중', '이', 'ㅂ니다', '!']\n"
     ]
    }
   ],
   "source": [
    "kor = kor_tagger.morphs(\"안녕하세요! 저는 파이토치를 공부하는 중입니다!\")\n",
    "print(kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'안녕': 0, '하': 1, '세요': 2, '!': 3, '저': 4, '는': 5, '파이': 6, '토치': 7, '를': 8, '공부': 9, '중': 10, '이': 11, 'ㅂ니다': 12}\n"
     ]
    }
   ],
   "source": [
    "# dictionary for indexing\n",
    "word2idx = {}\n",
    "for token in kor:\n",
    "    if word2idx.get(token) == None:\n",
    "        word2idx[token] = len(word2idx) # 순차적으로 token에 index을 할당함\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to one-hot encoding\n",
    "def one_hot_encoding(word, word2idx):\n",
    "    tensor = torch.zeros(len(word2idx))\n",
    "    index = word2idx[word]\n",
    "    tensor[index] = 1. # 단어에 해당하는 index에만 1을 할당. 나머지는 모두 0\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
      "         0.])\n",
      "torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "vec = one_hot_encoding(\"토치\", word2idx)\n",
    "print(vec)\n",
    "print(vec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.])\n",
      "torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "vec = one_hot_encoding(\"파이\", word2idx)\n",
    "print(vec)\n",
    "print(vec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장과 해당 문장의 카테고리를 나열한 데이터\n",
    "train_data = [[\"배고프다 밥줘\",\"FOOD\"],\n",
    "                    [\"뭐 먹을만한거 없냐\",\"FOOD\"],\n",
    "                    [\"맛집 추천\",\"FOOD\"],\n",
    "                    [\"이 근처 맛있는 음식점 좀\",\"FOOD\"],\n",
    "                    [\"밥줘\",\"FOOD\"],\n",
    "                    [\"뭐 먹지?\",\"FOOD\"],\n",
    "                    [\"삼겹살 먹고싶어\",\"FOOD\"],\n",
    "                    [\"영화 보고싶다\",\"MEDIA\"],\n",
    "                    [\"요즘 볼만한거 있어?\",\"MEDIA\"],\n",
    "                    [\"영화나 예능 추천\",\"MEDIA\"],\n",
    "                    [\"재밌는 드라마 보여줘\",\"MEDIA\"],\n",
    "                    [\"신과 함께 줄거리 좀 알려줘\",\"MEDIA\"],\n",
    "                    [\"고등랩퍼 다시보기 좀\",\"MEDIA\"],\n",
    "                    [\"재밌는 영상 하이라이트만 보여줘\",\"MEDIA\"]]\n",
    "\n",
    "test_data = [[\"쭈꾸미 맛집 좀 찾아줘\",\"FOOD\"],\n",
    "                   [\"매콤한 떡볶이 먹고싶다\",\"FOOD\"],\n",
    "                   [\"강남 씨지비 조조 영화 스케줄표 좀\",\"MEDIA\"],\n",
    "                   [\"효리네 민박 보고싶엉\",\"MEDIA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('배고프다 밥줘', '뭐 먹을만한거 없냐', '맛집 추천', '이 근처 맛있는 음식점 좀', '밥줘', '뭐 먹지?', '삼겹살 먹고싶어', '영화 보고싶다', '요즘 볼만한거 있어?', '영화나 예능 추천', '재밌는 드라마 보여줘', '신과 함께 줄거리 좀 알려줘', '고등랩퍼 다시보기 좀', '재밌는 영상 하이라이트만 보여줘')\n",
      "('FOOD', 'FOOD', 'FOOD', 'FOOD', 'FOOD', 'FOOD', 'FOOD', 'MEDIA', 'MEDIA', 'MEDIA', 'MEDIA', 'MEDIA', 'MEDIA', 'MEDIA')\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = list(zip(*train_data))\n",
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['배고프', '다', '밥', '주', '어'], ['뭐', '먹', '을', '만하', 'ㄴ', '거', '없', '냐'], ['맛', '집', '추천'], ['이', '근처', '맛있', '는', '음식', '점', '좀'], ['밥', '주', '어'], ['뭐', '먹', '지', '?'], ['삼겹살', '먹', '고', '싶', '어'], ['영화', '보', '고', '싶', '다'], ['요즘', '볼만', '하', 'ㄴ', '거', '있', '어', '?'], ['영화', '나', '예능', '추천'], ['재밌', '는', '드라마', '보여주', '어'], ['신', '과', '함께', '줄거리', '좀', '알려주', '어'], ['고등', '랩', '푸', '어', '다시', '보', '기', '좀'], ['재밌', '는', '영상', '하이라이트', '만', '보여주', '어']]\n"
     ]
    }
   ],
   "source": [
    "# train_x 데이터를 형태소 단위로 분리\n",
    "train_x = [kor_tagger.morphs(x) for x in train_x]\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<unk>': 0, '배고프': 1, '다': 2, '밥': 3, '주': 4, '어': 5, '뭐': 6, '먹': 7, '을': 8, '만하': 9, 'ㄴ': 10, '거': 11, '없': 12, '냐': 13, '맛': 14, '집': 15, '추천': 16, '이': 17, '근처': 18, '맛있': 19, '는': 20, '음식': 21, '점': 22, '좀': 23, '지': 24, '?': 25, '삼겹살': 26, '고': 27, '싶': 28, '영화': 29, '보': 30, '요즘': 31, '볼만': 32, '하': 33, '있': 34, '나': 35, '예능': 36, '재밌': 37, '드라마': 38, '보여주': 39, '신': 40, '과': 41, '함께': 42, '줄거리': 43, '알려주': 44, '고등': 45, '랩': 46, '푸': 47, '다시': 48, '기': 49, '영상': 50, '하이라이트': 51, '만': 52}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word-index dictionary\n",
    "word2idx = {'<unk>':0}\n",
    "\n",
    "for x in train_x:\n",
    "    for token in x:\n",
    "        if word2idx.get(token) == None:\n",
    "            word2idx[token] = len(word2idx)\n",
    "print(word2idx)\n",
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FOOD': 0, 'MEDIA': 1}\n"
     ]
    }
   ],
   "source": [
    "# class-index dict\n",
    "class2idx = {'FOOD':0, \"MEDIA\":1}\n",
    "print(class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(word2idx.get(\"패스트\")) # train data에 존재하지 않는 형태소이기 때문에 none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장과 word2idx이 주어진 문장 내의 단어를 onehot으로 변환\n",
    "def make_bag_of_words(seq, word2idx):\n",
    "    tensor = torch.zeros(len(word2idx))\n",
    "    for w in seq:\n",
    "        idx = word2idx.get(w)\n",
    "        \n",
    "        # 형태소 매칭 dictionary에 단어가 존재하는 경우\n",
    "        if idx != None:\n",
    "            tensor[idx] += 1. # onehot\n",
    "        # 형태소 매칭 dictionary에 단어가 존재하지 않는 경우\n",
    "        else:\n",
    "            idx = word2idx['<unk>'] # 0\n",
    "            tensor[idx] += 1.\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 53])\n"
     ]
    }
   ],
   "source": [
    "# 각 문장 데이터에 대해 onehot 변환\n",
    "train_x = torch.cat([make_bag_of_words(x, word2idx).to(device).view(1, -1) for x in train_x])\n",
    "print(train_x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n"
     ]
    }
   ],
   "source": [
    "# 각 문장 카테고리에 대해 변환\n",
    "train_y = torch.cat([torch.LongTensor([class2idx[y]]).to(device) for y in train_y])\n",
    "print(train_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장이 input으로 들어오면 해당 문장의 카테고리를 예측하는 classifier\n",
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(vocab_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 과정\n",
    "n_epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "model = BOWClassifier(len(word2idx), len(class2idx)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7054248452186584\n",
      "0.5429993271827698\n",
      "0.4370329678058624\n",
      "0.36316439509391785\n",
      "0.3090343177318573\n",
      "0.267861932516098\n",
      "0.23562932014465332\n",
      "0.20980529487133026\n",
      "0.1887173354625702\n",
      "0.17121779918670654\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    model.zero_grad()\n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.item())\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'FOOD', 1: 'MEDIA'}\n"
     ]
    }
   ],
   "source": [
    "idx2class = {v:k for k, v in class2idx.items()}\n",
    "print(idx2class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['쭈꾸미 맛집 좀 찾아줘', 'FOOD'], ['매콤한 떡볶이 먹고싶다', 'FOOD'], ['강남 씨지비 조조 영화 스케줄표 좀', 'MEDIA'], ['효리네 민박 보고싶엉', 'MEDIA']]\n"
     ]
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6404]) tensor([ 0])\n",
      "inputs: 쭈꾸미 맛집 좀 찾아줘\n",
      "prediction: FOOD\n",
      "truth:FOOD\n",
      "\n",
      "\n",
      "tensor([ 0.2854]) tensor([ 0])\n",
      "inputs: 매콤한 떡볶이 먹고싶다\n",
      "prediction: FOOD\n",
      "truth:FOOD\n",
      "\n",
      "\n",
      "tensor([ 0.9578]) tensor([ 1])\n",
      "inputs: 강남 씨지비 조조 영화 스케줄표 좀\n",
      "prediction: MEDIA\n",
      "truth:MEDIA\n",
      "\n",
      "\n",
      "tensor([ 0.6815]) tensor([ 1])\n",
      "inputs: 효리네 민박 보고싶엉\n",
      "prediction: MEDIA\n",
      "truth:MEDIA\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing 과정\n",
    "for test in test_data:\n",
    "    x = kor_tagger.morphs(test[0]) # x에 대해 형태소 분리\n",
    "    x = make_bag_of_words(x, word2idx).view(1, -1).to(device)\n",
    "    \n",
    "    outputs = model(x)\n",
    "    outputs, idx = torch.max(outputs, 1)\n",
    "    \n",
    "    print(outputs, idx)\n",
    "    \n",
    "    print(\"inputs: %s\" % test[0])\n",
    "    print(\"prediction: %s\" % idx2class[idx.item()])\n",
    "    print(\"truth:%s\" % test[1])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
